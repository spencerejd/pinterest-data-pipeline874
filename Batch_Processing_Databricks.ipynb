{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94a7bcf-e92f-420b-8e1f-d50069444845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read json files containing Pinterest data into DataFrames\n",
    "df_pin = spark.read.json(\"/mnt/0a1667ad2f7f/topics/0a1667ad2f7f.pin/partition=0/\")\n",
    "df_geo = spark.read.json(\"/mnt/0a1667ad2f7f/topics/0a1667ad2f7f.geo/partition=0/\")\n",
    "df_user = spark.read.json(\"/mnt/0a1667ad2f7f/topics/0a1667ad2f7f.user/partition=0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e206319c-6d0a-40d7-bcda-bce04af00b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin.printSchema()\n",
    "df_geo.printSchema()\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67b5f4a-5907-4106-9f2c-f3b528f12813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 1: Clean the df_pin DataFrame\n",
    "\n",
    "# Replace empty entries and entries that do not contain relevant data in each column with None\n",
    "cleaned_df_pin = (df_pin.replace({'No description available Story format': None}, subset=['description'])\n",
    "                    .replace({'No description available': None}, subset=['description'])\n",
    "                    .replace({'User Info Error': None}, subset=['follower_count'])\n",
    "                    .replace({'Image src Error.': None}, subset=['image_src'])\n",
    "                    .replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None}, subset=['tag_list'])\n",
    "                    .replace({'No Title Data Available': None}, subset=['title'])\n",
    "                    .replace({'User Info Error': None}, subset=['poster_name']))\n",
    "\n",
    "# Perform necessary transformations on the follower_count to ensure every entry is a number.\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# Make sure the data type of this column is an integer\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", cleaned_df_pin[\"follower_count\"].cast(\"integer\"))\n",
    "\n",
    "# Ensure each column containing numeric data has a numeric data type\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"downloaded\", cleaned_df_pin[\"downloaded\"].cast(\"integer\"))\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"index\", cleaned_df_pin[\"index\"].cast(\"integer\"))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "# Rename the index column to ind\n",
    "cleaned_df_pin = cleaned_df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "cleaned_df_pin = cleaned_df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "# This removes the 'download' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5ae802-2bcf-45a9-b550-96413ea34601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 2: Clean df_geo DataFrame\n",
    "\n",
    "# Create new column 'coordinates' containing an array based on the 'latitude' and 'longitude' columns\n",
    "from pyspark.sql.functions import array\n",
    "cleaned_df_geo = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "\n",
    "# Drop the latitude and longitude columns from DataFrame\n",
    "cleaned_df_geo = (cleaned_df_geo.drop(\"latitude\")\n",
    "                                .drop(\"longitude\"))\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df_geo = cleaned_df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "cleaned_df_geo = cleaned_df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cbb579-4bad-4099-983f-663786a94458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3: Clean df_user DataFrame\n",
    "\n",
    "# Create new column user_name that concatenates the first_name and last_name columns\n",
    "from pyspark.sql.functions import concat\n",
    "cleaned_df_user = df_user.withColumn(\"user_name\", concat(\"first_name\", \"last_name\"))\n",
    "\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "cleaned_df_user = (cleaned_df_user.drop(\"first_name\")\n",
    "                                    .drop(\"last_name\"))\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df_user = cleaned_df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "cleaned_df_user = cleaned_df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ab67f8-38c7-4864-92f8-cf71d133bae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_df_pin)\n",
    "display(cleaned_df_geo)\n",
    "display(cleaned_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7807e41c-09f2-4402-a30c-b9311ff9aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_pin.printSchema()\n",
    "cleaned_df_geo.printSchema()\n",
    "cleaned_df_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015c1511-6cd8-410b-ac31-2621744613da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 4: Find the most popular category in each country\n",
    "\n",
    "# Join pin and geo DataFrames\n",
    "combined_df_t4 = cleaned_df_pin.join(cleaned_df_geo, cleaned_df_pin[\"ind\"] == cleaned_df_geo[\"ind\"], how=\"inner\")\n",
    "\n",
    "# Identify most popular category in each country\n",
    "from pyspark.sql.functions import col, count, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# # Use groupBy and aggregation functions\n",
    "# grouped_df_t4 = combined_df_t4.groupBy([\"country\", \"category\"]).agg(count(\"category\").alias(\"category_count\"))\n",
    "# grouped_df_t4 = grouped_df_t4.orderBy([\"country\", \"category_count\"], ascending=[True, False])\n",
    "# # This does not quite work - you can't filter to keep just the top category\n",
    "\n",
    "# Using window function to present only the most popular category\n",
    "\n",
    "grouped_df_t4 = (combined_df_t4.groupBy([\"country\", \"category\"])\n",
    "                                .agg(count(\"category\").alias(\"category_count\")))\n",
    "# Create Window specification\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "# Apply window function\n",
    "grouped_df_t4 = grouped_df_t4.withColumn(\"rank\", rank().over(window_spec))\n",
    "# Filter to keep only top category in each country\n",
    "grouped_df_t4 = grouped_df_t4.filter(col(\"rank\") == 1).drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdbde32-b99c-4610-8813-344e1ba8f5f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grouped_df_t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70a9c6a-c08a-466e-a712-973c764c3a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 5: Find which was the most popular category each year\n",
    "\n",
    "# Join pin and geo DataFrames\n",
    "combined_df_t5 = cleaned_df_pin.join(cleaned_df_geo, cleaned_df_pin[\"ind\"] == cleaned_df_geo[\"ind\"], how=\"inner\")\n",
    "\n",
    "# Identify most popular category each year\n",
    "from pyspark.sql.functions import year, col, count, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create post_year column\n",
    "combined_df_t5 = combined_df_t5.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Filter to restrict post_year to between 2018 and 2022\n",
    "combined_df_t5 = combined_df_t5.filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022))\n",
    "\n",
    "# # Use groupBy and aggregation functions\n",
    "grouped_df_t5 = (combined_df_t5.groupBy([\"post_year\", \"category\"])\n",
    "                            .agg(count(\"category\").alias(\"category_count\")))\n",
    "\n",
    "# Create a Window specification\n",
    "window_spec = Window.partitionBy(\"post_year\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Apply window function to rank yearly category posts\n",
    "grouped_df_t5 = grouped_df_t5.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter to keep only top category in each year\n",
    "grouped_df_t5 = grouped_df_t5.filter(col(\"rank\") == 1).drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e9f497-1c68-4b5b-b8df-23fb1835deaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grouped_df_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cff42b-5240-4f7c-b40c-5c582fc5539d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6: Find the user with most followers in each country\n",
    "\n",
    "# Step 1: For each country find the user with the most followers\n",
    "from pyspark.sql.functions import max, desc\n",
    "# Join cleaned pin and geo DataFrames\n",
    "combined_df_t6 = cleaned_df_pin.alias(\"pin\").join(cleaned_df_geo.alias(\"geo\"), cleaned_df_pin[\"ind\"] == cleaned_df_geo[\"ind\"], how=\"inner\")\n",
    "\n",
    "# Aggregation to find maximum follower count per country\n",
    "max_follower_per_country = combined_df_t6.groupBy([\"geo.country\"]).agg(max(\"pin.follower_count\").alias(\"max_follower_count\"))\n",
    "\n",
    "# Rename country_column in max_follower_per_country to avoid naming conflict later\n",
    "max_follower_per_country = max_follower_per_country.withColumnRenamed(\"country\", \"country_agg\")\n",
    "\n",
    "# Join max_follower_per_country DataFrame with combined_df_t6 to get poster_name for each maximum follower count\n",
    "df_with_max_follower = combined_df_t6.join(max_follower_per_country,\n",
    "                                           (combined_df_t6[\"geo.country\"] == max_follower_per_country[\"country_agg\"]) & (combined_df_t6[\"pin.follower_count\"] == max_follower_per_country[\"max_follower_count\"]), how=\"inner\")\n",
    "\n",
    "# Select desired columns\n",
    "result_df_step1 = df_with_max_follower.select(\"country\", \"poster_name\", \"follower_count\").distinct()\n",
    "\n",
    "# Step 2: Based on the above query, find the country with the user with most followers\n",
    "# Return a DataFrame with the following columns: country, follower_count\n",
    "grouped_df_step2 = result_df_step1.groupBy(\"country\").agg(max(\"follower_count\").alias(\"follower_count\"))\n",
    "\n",
    "# Order grouped_df_step2 by follower_count\n",
    "ordered_df_step2 = grouped_df_step2.orderBy(desc(\"follower_count\"))\n",
    "\n",
    "# Select country with highest follower_count\n",
    "result_df_step2 = ordered_df_step2.select([\"country\", \"follower_count\"]).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a12f74-9d09-47c8-a68b-e18a407f1b1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_df_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b80c6e7-b847-4b6d-99f7-6bc846f2793d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 7: Find the most popular category for different age groups\n",
    "#  18-24, 25-35, 36-50, +50\n",
    "# Query should return a DataFrame containing the following columns;\n",
    "# age_group, category, category_count\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "# Join pin and user DataFrames\n",
    "combined_df_t7 = cleaned_df_pin.join(cleaned_df_user, cleaned_df_pin[\"ind\"] == cleaned_df_user[\"ind\"], \"inner\")\n",
    "# Select relevant columns\n",
    "combined_df_t7 = combined_df_t7.select(\"age\", \"category\")\n",
    "# Categorise age into ranges\n",
    "combined_df_t7 = combined_df_t7.withColumn(\"age_group\",\n",
    "                                           when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "                                           .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "                                           .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "                                           .otherwise(\"+50\"))\n",
    "\n",
    "# Group by age_group and count the categories\n",
    "grouped_df_t7 = combined_df_t7.groupBy([\"age_group\", \"category\"]).agg(count(\"category\").alias(\"category_count\"))\n",
    "\n",
    "# Find the most popular category within each age group\n",
    "# Rank and filter each age group\n",
    "\n",
    "# Create window specification that partitions data by age_group and orders by category_count\n",
    "window_spec = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "# Apply window function\n",
    "grouped_df_t7 = grouped_df_t7.withColumn(\"rank\", rank().over(window_spec))\n",
    "# Filter for most popular category for each age group\n",
    "result_df_t7 = grouped_df_t7.filter(col(\"rank\") == 1).drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b4c6c1-eba0-4f29-bd63-c4d153ecb31f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_df_t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9767b5-2c32-4831-8452-0d577bec3fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 8: Find the median follower count for different age groups\n",
    "# What is the median follower count for users in the following age groups:\n",
    "#  18-24, 25-35, 36-50, +50\n",
    "# Query should return DataFrame containing age_group, median_follower_count\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "# Join pin and user DataFrames\n",
    "combined_df_t8 = cleaned_df_pin.join(cleaned_df_user, cleaned_df_pin[\"ind\"] == cleaned_df_user[\"ind\"], \"inner\")\n",
    "# Categorise age into ranges\n",
    "combined_df_t8 = combined_df_t8.withColumn(\"age_group\",\n",
    "                                           when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "                                           .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "                                           .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "                                           .otherwise(\"+50\"))\n",
    "\n",
    "# Calculate median for each age group\n",
    "age_groups = [\"18-24\", \"25-35\", \"36-50\", \"+50\"]\n",
    "medians = []\n",
    "for group in age_groups:\n",
    "    # Filter DataFrame for current age group in for loop\n",
    "    temp_df = combined_df_t8.filter(col(\"age_group\") == group)\n",
    "    # Calculate median follower count for current age group\n",
    "    median = temp_df.stat.approxQuantile(\"follower_count\", [0.5], 0.01)\n",
    "    # Store result\n",
    "    medians.append((group, median[0]))\n",
    "\n",
    "# Convert list of medians into DataFrame\n",
    "median_df = spark.createDataFrame(medians, [\"age_group\", \"median_follower_count\"])\n",
    "\n",
    "# # Select relevant columns\n",
    "# combined_df_t8 = combined_df_t8.select(\"age_group\", \"follower_count\")\n",
    "\n",
    "# # Group by age_group and aggregate by median follower_count\n",
    "# grouped_df_t8 = combined_df_t8.groupBy(\"age_group\").agg(max(\"follower_count\")-(min(\"follower_count\"))).alias(\"median_follower_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80000f25-95fa-4d33-a42d-b72fd361c536",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(median_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4557b919-deac-4fba-9331-553bfcead424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 9: Find how many users have joined each year (2015-2020)?\n",
    "# Query should return DataFrame that contains post_year and number_users_joined\n",
    "\n",
    "# Join pin and user DataFrames\n",
    "combined_df_t9 = cleaned_df_pin.join(cleaned_df_user, cleaned_df_pin[\"ind\"] == cleaned_df_user[\"ind\"], \"inner\")\n",
    "# Select relevant columns\n",
    "combined_df_t9 = combined_df_t9.select(\"date_joined\")\n",
    "# Extract year\n",
    "task9_df = combined_df_t9.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "# Group and aggregate\n",
    "task9_df = task9_df.groupBy(\"post_year\").agg(count(\"date_joined\").alias(\"number_users_joined\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5969a530-7821-44e2-a63f-b9c7de827c78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(task9_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4816eafb-cf59-4a2b-a638-131ecfdd85ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 10: Find the median follower count of users based on their joining year (2015-2020)\n",
    "#  Query should return DataFrame containing post_year and median_follower_count\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Join pin and user DataFrames\n",
    "combined_df_t10 = cleaned_df_pin.join(cleaned_df_user, cleaned_df_pin[\"ind\"] == cleaned_df_user[\"ind\"], \"inner\")\n",
    "# Select relevant columns\n",
    "combined_df_t10 = combined_df_t10.select(\"date_joined\", \"follower_count\")\n",
    "# Extract year\n",
    "task10_df = combined_df_t10.withColumn(\"post_year\", year(\"date_joined\")).drop(\"date_joined\")\n",
    "\n",
    "# Calculate median follower count using approxQuantile\n",
    "post_years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "medians = []\n",
    "for year in post_years:\n",
    "    try:\n",
    "        # Filter DataFrame for the current posting year\n",
    "        temp_df = task10_df.filter(col(\"post_year\") == year)\n",
    "\n",
    "        # Calculate median follower count for current posting year\n",
    "        median = temp_df.stat.approxQuantile(\"follower_count\", [0.5], 0.01)\n",
    "        # Store the result\n",
    "        medians.append((year, median[0]))\n",
    "    # Append None if median is null\n",
    "    except IndexError:\n",
    "        medians.append((year, None))\n",
    "\n",
    "median_df_t10 = spark.createDataFrame(medians, [\"post_year\", \"median_follower_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ef2412-8384-4e86-a579-f3f006b8d7b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(median_df_t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2cc5b54-c584-444b-86c9-2ee16b4fb412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 11: Find the median follower count of users based on their joining year and age group\n",
    "# 2015-2020, [18-24, 25-35, 36-50, +50]\n",
    "# Your query should return age_group, post_year, median_follower_count\n",
    "\n",
    "from pyspark.sql.functions import year, col\n",
    "# Join pin and user DataFrames\n",
    "combined_df_t11 = (cleaned_df_pin.join(cleaned_df_user, cleaned_df_pin[\"ind\"] == cleaned_df_user[\"ind\"], \"inner\")\n",
    "                                .join(cleaned_df_geo, cleaned_df_pin[\"ind\"] == cleaned_df_geo[\"ind\"], \"inner\"))\n",
    "# Extract year from timestamp column\n",
    "combined_df_t11 = combined_df_t11.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "# Create age_group column\n",
    "task11_df = combined_df_t11.withColumn(\"age_group\",\n",
    "                                           when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "                                           .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "                                           .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "                                           .otherwise(\"+50\"))\n",
    "# Select relevant columns\n",
    "task11_df = task11_df.select(\"age_group\",\"post_year\", \"follower_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c555295-f169-4ba8-b3d7-afb7a5bcd798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate median_follower_count using approxQuantile\n",
    "post_years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "age_groups = [\"18-24\", \"25-35\", \"36-50\", \"+50\"]\n",
    "medians = []\n",
    "for year in post_years:\n",
    "    # Filter DataFrame for the current posting year\n",
    "    year_df = task11_df.filter(col(\"post_year\") == year)\n",
    "    for age in age_groups:\n",
    "        try:\n",
    "            # Filter DataFrame for the current age group\n",
    "            age_df = year_df.filter(col(\"age_group\") == age)\n",
    "\n",
    "            # Calculate median follower count for current age group\n",
    "            median = age_df.stat.approxQuantile(\"follower_count\", [0.5], 0.01)\n",
    "\n",
    "            # Store the result\n",
    "            medians.append((year, age, median[0]))\n",
    "        except IndexError:\n",
    "            medians.append((year, age, None))\n",
    "\n",
    "median_df_t11 = spark.createDataFrame(medians, [\"post_year\", \"age_group\", \"median_follower_count\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4610a75b-a571-47c2-9567-94528927ba85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(median_df_t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e2031d-f885-4db5-85b9-9969e3654783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_df_pin)\n",
    "display(cleaned_df_geo)\n",
    "display(cleaned_df_user)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Milestone_7_Batch_Processing_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
