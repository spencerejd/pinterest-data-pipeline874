{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd60b98a-644e-4ddc-98ae-c276dd200bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d146c0f9-1a9c-4a47-a16d-77691a0742e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e588876-4e62-4eb0-974c-70fdf84d5ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Kinesis streaming data into Databricks Notebook\n",
    "\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a1667ad2f7f-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a1667ad2f7f-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a1667ad2f7f-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2df0d2-f8f1-4455-8c63-de25cba36350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Â Deserialize streaming data\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "#Â Parse JSON Data into format suitable for PySpark\n",
    "# from pyspark.sql.functions import from_json\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "#Â Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),   \n",
    "    StructField(\"downloaded\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "# Parse JSON data\n",
    "df_pin = df_pin.withColumn(\"jsonData\", from_json(\"data\", schema)).select(\"jsonData.*\")\n",
    "df_pin = df_pin.dropna(how=\"all\")\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57dcbb54-4d8e-42ae-b2ca-8c3ce16113a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean the df_pin DataFrame\n",
    "\n",
    "# Replace empty entries and entries that do not contain relevant data in each column with None\n",
    "cleaned_df_pin = (df_pin.replace({'No description available Story format': None}, subset=['description'])\n",
    "                    .replace({'No description available': None}, subset=['description'])\n",
    "                    #.replace({'Untitled': None}, subset=['description']) # Unsure of this one ðŸ¤”\n",
    "                    .replace({'User Info Error': None}, subset=['follower_count'])\n",
    "                    .replace({'Image src Error.': None}, subset=['image_src'])\n",
    "                    .replace({'Image src error.': None}, subset=['image_src'])\n",
    "                    .replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None}, subset=['tag_list'])\n",
    "                    .replace({'No Title Data Available': None}, subset=['title'])\n",
    "                    .replace({'User Info Error': None}, subset=['poster_name']))\n",
    "\n",
    "#Â Perform necessary transformations on the follower_count to ensure every entry is a number.\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# Make sure the data type of this column is an integer\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", cleaned_df_pin[\"follower_count\"].cast(\"integer\"))\n",
    "\n",
    "# Ensure each column containing numeric data has a numeric data type\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"downloaded\", cleaned_df_pin[\"downloaded\"].cast(\"integer\"))\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"index\", cleaned_df_pin[\"index\"].cast(\"integer\"))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "# Rename the index column to ind\n",
    "cleaned_df_pin = cleaned_df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "cleaned_df_pin = cleaned_df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\")\n",
    "# This removes the 'download' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c59f1c1-68ef-4e7d-9309-e10148f48cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562cd608-b841-49d4-8432-cfd269795809",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_pin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e5c6ae-fefb-4497-9172-35be3dd54d06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Â Deserialize streaming data\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "#Â Parse JSON Data into format suitable for PySpark\n",
    "# from pyspark.sql.functions import from_json\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "#Â Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "# Parse JSON data\n",
    "df_geo = df_geo.withColumn(\"jsonData\", from_json(\"data\", schema)).select(\"jsonData.*\")\n",
    "df_geo = df_geo.dropna(how=\"all\")\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c87702-4a57-4911-9cce-d09d8ca4a5a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean df_geo DataFrame\n",
    "\n",
    "# Create new column 'coordinates' containing an array based on the 'latitude' and 'longitude' columns\n",
    "from pyspark.sql.functions import array\n",
    "cleaned_df_geo = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "\n",
    "#Â Drop the latitude and longitude columns from DataFrame\n",
    "cleaned_df_geo = (cleaned_df_geo.drop(\"latitude\")\n",
    "                                .drop(\"longitude\"))\n",
    "\n",
    "#Â Convert the timestamp column from a string to a timestamp data type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df_geo = cleaned_df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "cleaned_df_geo = cleaned_df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3172a51-1701-44f6-9ca2-e12b9808672d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df6d06e-985e-4c03-90ef-cb9bed0a453a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407544f5-244d-4442-99c2-223ead61668c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Â Deserialize streaming data\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "#Â Parse JSON Data into format suitable for PySpark\n",
    "# from pyspark.sql.functions import from_json\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "#Â Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True)\n",
    "])\n",
    "# Parse JSON data\n",
    "df_user = df_user.withColumn(\"jsonData\", from_json(\"data\", schema)).select(\"jsonData.*\")\n",
    "df_user = df_user.dropna(how=\"all\")\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a53bda-738f-4372-9c25-e8607557b427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean df_user DataFrame\n",
    "\n",
    "# Create new column user_name that concatenates the first_name and last_name columns\n",
    "from pyspark.sql.functions import concat\n",
    "cleaned_df_user = df_user.withColumn(\"user_name\", concat(\"first_name\", \"last_name\"))\n",
    "\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "cleaned_df_user = (cleaned_df_user.drop(\"first_name\")\n",
    "                                    .drop(\"last_name\"))\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df_user = cleaned_df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "#Â Reorder the DataFrame columns\n",
    "cleaned_df_user = cleaned_df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe4f508-e2e0-4871-95b8-b7d83e36f78c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_user.printSchema()\n",
    "display(cleaned_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6733cb8a-d298-4f8b-9d10-6e63ddc70d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9653aa-0c10-42a6-b8a7-fcc47f438677",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0a1667ad2f7f_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6ecccf-366d-4270-a53d-14b7022635e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0a1667ad2f7f_geo_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec0aadd-7fad-45ce-aec8-ed8f682d0b6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0a1667ad2f7f_user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b60c59-e66a-4846-a342-56d7a0deb689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1143679294347450,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stream_Processing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
